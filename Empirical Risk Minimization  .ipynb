{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "21224317",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [1, 2, 3, 4, 5]\n",
    "y_train = [2, 4, 6, 8, 10]\n",
    "\n",
    "weight = 0\n",
    "bias = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2b6cdb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(X_train, y_train, learning_rate=0.01, epochs=100):\n",
    "    n = len(X_train)\n",
    "    for _ in range(epochs):\n",
    "        predictions = [predict(x) for x in X_train]\n",
    "        errors = [predictions[i] - y_train[i] for i in range(n)]\n",
    "        d_weight = sum([errors[i] * X_train[i] for i in range(n)]) / n\n",
    "        d_bias = sum(errors) / n\n",
    "        update_parameters(d_weight, d_bias, learning_rate)\n",
    "        total_loss = calculate_total_loss(errors)\n",
    "        print(f'Loss in epoch {_+1}: {total_loss}')\n",
    "\n",
    "def predict(x):\n",
    "    return weight * x + bias\n",
    "\n",
    "def update_parameters(d_weight, d_bias, learning_rate):\n",
    "    global weight, bias\n",
    "    weight -= learning_rate * d_weight\n",
    "    bias -= learning_rate * d_bias\n",
    "\n",
    "def calculate_total_loss(errors):\n",
    "    n = len(errors)\n",
    "    total_loss = sum([error**2 for error in errors]) / n\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dd6532fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 1: 44.0\n",
      "Loss in epoch 2: 34.2152\n",
      "Loss in epoch 3: 26.60867504\n",
      "Loss in epoch 4: 20.695493521952006\n",
      "Loss in epoch 5: 16.098680239005766\n",
      "Loss in epoch 6: 12.525182919057132\n",
      "Loss in epoch 7: 9.747189141929951\n",
      "Loss in epoch 8: 7.587602519871062\n",
      "Loss in epoch 9: 5.9087521982751845\n",
      "Loss in epoch 10: 4.60361586876093\n",
      "Loss in epoch 11: 3.588996731713711\n",
      "Loss in epoch 12: 2.8002194163612173\n",
      "Loss in epoch 13: 2.187006705176271\n",
      "Loss in epoch 14: 1.7102741897092408\n",
      "Loss in epoch 15: 1.339638506215198\n",
      "Loss in epoch 16: 1.0514802925891804\n",
      "Loss in epoch 17: 0.8274383735004299\n",
      "Loss in epoch 18: 0.6532391728864961\n",
      "Loss in epoch 19: 0.5177867248569044\n",
      "Loss in epoch 20: 0.4124552680944243\n",
      "Loss in epoch 21: 0.3305393242317029\n",
      "Loss in epoch 22: 0.2668262008221385\n",
      "Loss in epoch 23: 0.217263664509741\n",
      "Loss in epoch 24: 0.17870159742787833\n",
      "Loss in epoch 25: 0.14869116654025927\n",
      "Loss in epoch 26: 0.1253287022839126\n",
      "Loss in epoch 27: 0.10713433324461479\n",
      "Loss in epoch 28: 0.09295763941093269\n",
      "Loss in epoch 29: 0.0819043090796894\n",
      "Loss in epoch 30: 0.07327912354016482\n",
      "Loss in epoch 31: 0.06654163461597211\n",
      "Loss in epoch 32: 0.061271709356580384\n",
      "Loss in epoch 33: 0.05714274523491408\n",
      "Loss in epoch 34: 0.05390084822844017\n",
      "Loss in epoch 35: 0.05134864631530275\n",
      "Loss in epoch 36: 0.049332706440632604\n",
      "Loss in epoch 37: 0.04773375274166318\n",
      "Loss in epoch 38: 0.046459062410108845\n",
      "Loss in epoch 39: 0.045436554402074385\n",
      "Loss in epoch 40: 0.04461019413056068\n",
      "Loss in epoch 41: 0.04393642117403633\n",
      "Loss in epoch 42: 0.043381372255292774\n",
      "Loss in epoch 43: 0.042918722446005544\n",
      "Loss in epoch 44: 0.04252800696642626\n",
      "Loss in epoch 45: 0.04219331658920361\n",
      "Loss in epoch 46: 0.04190228347487437\n",
      "Loss in epoch 47: 0.04164529278257551\n",
      "Loss in epoch 48: 0.041414869793471906\n",
      "Loss in epoch 49: 0.04120520347392166\n",
      "Loss in epoch 50: 0.04101177610391016\n",
      "Loss in epoch 51: 0.04083107535830076\n",
      "Loss in epoch 52: 0.04066037048509783\n",
      "Loss in epoch 53: 0.04049753831132639\n",
      "Loss in epoch 54: 0.04034092798380763\n",
      "Loss in epoch 55: 0.040189255821596485\n",
      "Loss in epoch 56: 0.040041523576564035\n",
      "Loss in epoch 57: 0.03989695489096085\n",
      "Loss in epoch 58: 0.0397549459009135\n",
      "Loss in epoch 59: 0.03961502683666074\n",
      "Loss in epoch 60: 0.03947683217141361\n",
      "Loss in epoch 61: 0.03934007741573028\n",
      "Loss in epoch 62: 0.039204541077965074\n",
      "Loss in epoch 63: 0.03907005064071507\n",
      "Loss in epoch 64: 0.038936471659211346\n",
      "Loss in epoch 65: 0.03880369928664358\n",
      "Loss in epoch 66: 0.03867165168612948\n",
      "Loss in epoch 67: 0.03854026490932021\n",
      "Loss in epoch 68: 0.038409488915135626\n",
      "Loss in epoch 69: 0.038279284474814365\n",
      "Loss in epoch 70: 0.03814962076596215\n",
      "Loss in epoch 71: 0.03802047350221398\n",
      "Loss in epoch 72: 0.037891823479272016\n",
      "Loss in epoch 73: 0.03776365544462229\n",
      "Loss in epoch 74: 0.037635957218874215\n",
      "Loss in epoch 75: 0.037508719012704715\n",
      "Loss in epoch 76: 0.037381932895862405\n",
      "Loss in epoch 77: 0.03725559238438022\n",
      "Loss in epoch 78: 0.037129692119678016\n",
      "Loss in epoch 79: 0.0370042276191016\n",
      "Loss in epoch 80: 0.036879195081993985\n",
      "Loss in epoch 81: 0.036754591238934084\n",
      "Loss in epoch 82: 0.036630413234535975\n",
      "Loss in epoch 83: 0.03650665853633668\n",
      "Loss in epoch 84: 0.036383324863961466\n",
      "Loss in epoch 85: 0.0362604101340575\n",
      "Loss in epoch 86: 0.03613791241748102\n",
      "Loss in epoch 87: 0.03601582990601366\n",
      "Loss in epoch 88: 0.035894160886483555\n",
      "Loss in epoch 89: 0.03577290372064455\n",
      "Loss in epoch 90: 0.03565205682953216\n",
      "Loss in epoch 91: 0.03553161868129709\n",
      "Loss in epoch 92: 0.03541158778174524\n",
      "Loss in epoch 93: 0.03529196266697897\n",
      "Loss in epoch 94: 0.035172741897674345\n",
      "Loss in epoch 95: 0.03505392405462854\n",
      "Loss in epoch 96: 0.034935507735294476\n",
      "Loss in epoch 97: 0.034817491551083615\n",
      "Loss in epoch 98: 0.03469987412526747\n",
      "Loss in epoch 99: 0.03458265409133944\n",
      "Loss in epoch 100: 0.03446583009174202\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train(X_train, y_train)\n",
    "\n",
    "X_test = 6\n",
    "predicted_value = predict(X_test)\n",
    "error = predicted_value - X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bdacc9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value when X is = 6 : 11.712420602693342\n",
      "Error between predicted value and actual value: 5.712420602693342\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Predicted value when X is =\", X_test, \":\", predicted_value)\n",
    "print(\"Error between predicted value and actual value:\", error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a61f026",
   "metadata": {},
   "source": [
    "Empirical Risk Minimization\" is a concept in the field of machine learning that focuses on developing automated models capable of performing well on the data they were trained on. This is achieved by minimizing the loss or gap between the predictions made by the models and the actual values in the training dataset. By doing so, the models aim to generalize well to new, unseen data, thus improving their overall performance and reliability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
